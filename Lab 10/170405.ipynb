{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b14463-2c4d-4a53-a8d1-def39d052cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 07 - Random Forest for Regression\n",
    "\n",
    "In this lab we will be extending the previous lab about Decision trees and build a Regression model using Random Forest.\n",
    "\n",
    "For simplicity, we will be using the same dataset as the previous lab (you can find it in ECLASS).\n",
    "\n",
    "**IMPORTANT:** For this lab, if you haven't finished your code from last week's lab on Decision trees, you will have the option to use the sklearn implementation for a regression tree. However, this doesn't mean that you should skip the previous lab. This is just so that you don't get behind with the content and you don't spend all your time today working on the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3323dc4-95cd-4564-8ccd-441019188fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29dfe0-10af-4871-ab66-e7cffe0685cc",
   "metadata": {},
   "source": [
    "As mentioned before, use the Boston Housing data and prepare your train/val/test split as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6c793-4f97-49ff-a1f3-3e5ec2aa086b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aaba32-4c00-404c-9899-7a5759059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, num_bags=10):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    * The\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) # you can change the seed, or use 0 to replicate my results\n",
    "    # Your code here\n",
    "\n",
    "    # Número de datapoints\n",
    "    n_of_datapoints = X.shape[0]\n",
    "    # Array com os índices dos dados\n",
    "    indices = np.array(range(n_of_datapoints))\n",
    "    # Lista com os bags criados\n",
    "    bags = []\n",
    "\n",
    "    # Para cada bag...\n",
    "    for _ in range(num_bags):\n",
    "        # Sorteia as amostras aleatoriamente\n",
    "        samples = rng.choice(indices, size = n_of_datapoints, replace = True)\n",
    "        # Adiciona-as na lista\n",
    "        bags.append(samples)\n",
    "\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff32d32b-f615-43da-b70e-aa8959d01093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 63, 51, 26, 30,  4,  7,  1, 17, 81, 64, 91, 50, 60, 97, 72, 63,\n",
       "       54, 55, 93, 27, 81, 67,  0, 39, 85, 55,  3, 76, 72, 84, 17,  8, 86,\n",
       "        2, 54,  8, 29, 48, 42, 40,  2,  0, 12,  0, 67, 52, 64, 25, 61, 76,\n",
       "       38, 46, 99, 80, 98, 37, 68, 95, 65, 84, 68, 70, 38, 87, 13, 57, 72,\n",
       "       84, 52, 37, 31, 42, 48, 71, 88,  7, 93, 53, 35, 67, 57, 25, 32, 71,\n",
       "       59, 50, 33, 76, 39, 32, 89, 26, 22, 71, 62,  4,  8, 37, 83])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b5126-5aad-4c74-a7de-ad3935acd7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baac85e0-2a36-4ee0-92a5-e31a01a70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    # Criando um único array com todas as predições\n",
    "    preds = np.array(preds)\n",
    "    # Calculando as previsões médias\n",
    "    mean_preds = np.mean(preds, axis = 0)\n",
    "\n",
    "    return mean_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f57a-6395-4285-ac01-e1099af84dde",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "For this part, you can use the sklearn implementation of Random forest for regression as your estimator for each set of features and bags. See below an example of how to do this, and always remember to check the necessary documentation when using an external function.\n",
    "\n",
    "Some parameters you will have to set are: \n",
    "* num_features: number of features per estimator\n",
    "* min_samples: min number of samples per leaf node\n",
    "* max_depth: maximum depth of the decision tree (each estimator)\n",
    "* num_estimators: number of decision trees you will create using each bag and random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71136bc0-c316-41d9-8832-c2db217101ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of sklearn Decision tree\n",
    "# estimator = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "# estimator.fit(X, y)\n",
    "# estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ecbd9d-9628-42d4-88ee-c3c3ea408e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:\n",
    "\n",
    "# Carregando e separando os dados\n",
    "data = pd.read_csv(\"BostonHousing.txt\")\n",
    "\n",
    "y = data[\"medv\"].to_numpy()\n",
    "X = data.drop(\"medv\", axis = 1).to_numpy()\n",
    "\n",
    "X_train_and_val, X_test, y_train_and_val, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_and_val, y_train_and_val, test_size = 1/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbb5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o erro de criterion de uma região\n",
    "def regression_criterion(region):\n",
    "    # Se a região for vazia, o erro é infinito\n",
    "    if region.size == 0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    # Calculando a média da região\n",
    "    y_hat = np.mean(region)\n",
    "    # Calculando o erro da região\n",
    "    region_error = np.sum((region - y_hat)**2)\n",
    "\n",
    "    return region_error\n",
    "\n",
    "\n",
    "# Função para dividir uma região\n",
    "def split_region(region, feature_index, tau):\n",
    "    # Array de booleanos para filtrar os registros\n",
    "    split = region[:, feature_index] < tau\n",
    "    # Array com os índices dos registros nessa região\n",
    "    indices = np.array(range(region.shape[0]))\n",
    "\n",
    "    # Encontrando as partições\n",
    "    left_partition = indices[split]\n",
    "    right_partition = indices[~split]\n",
    "\n",
    "    return left_partition, right_partition\n",
    "\n",
    "\n",
    "# Função para obter a melhor divisão de uma região\n",
    "def get_split(X, y, available_features):\n",
    "    # Inicializando as melhores configurações de divisão\n",
    "    best_feature = 0\n",
    "    best_tau = 0\n",
    "    best_criterion = float(\"inf\")\n",
    "\n",
    "    # Para cada feature...\n",
    "    for feature in available_features:\n",
    "        # Cria a lista dos taus a serem testados, sendo os valores desses dados para essa feature\n",
    "        # adicionado com um valor maior que todos eles\n",
    "        taus = np.unique(X[:, feature])\n",
    "        taus = np.append(taus, np.max(taus) + 1)\n",
    "\n",
    "        # Para cada tau...\n",
    "        for tau in taus:\n",
    "            # Divide a região\n",
    "            left, right = split_region(X, feature, tau)\n",
    "            # Calcula o criterion somando os criterions das duas regiões criadas\n",
    "            criterion = regression_criterion(y[left]) + regression_criterion(y[right])\n",
    "\n",
    "            # Se esse criterion for menor que o melhor até agora...\n",
    "            if criterion < best_criterion:\n",
    "                # Atualiza as melhores configurações para essa\n",
    "                best_feature = feature\n",
    "                best_tau = tau\n",
    "                best_criterion = criterion\n",
    "\n",
    "    # Dividindo a região com as melhores configurações\n",
    "    left_region, right_region = split_region(X, best_feature, best_tau)\n",
    "\n",
    "    # Criando o dicionário com as informações solicitadas\n",
    "    decision = {\"feature_index\": best_feature, \"tau\": best_tau, \"left_region\": left_region, \"right_region\": right_region}\n",
    "\n",
    "    return decision\n",
    "\n",
    "\n",
    "# Função para crescer uma árvore recursivamente\n",
    "def recursive_growth(node, min_samples, max_depth, current_depth, X, y, available_features):\n",
    "    # Para cada região...\n",
    "    for region in [\"left_region\", \"right_region\"]:\n",
    "        # Pega os índices dos registros dessa região\n",
    "        region_indices = node[region]\n",
    "        # Se algum dos critérios de parada for atendido...\n",
    "        if region_indices.size <= min_samples or current_depth == max_depth or regression_criterion(y[region_indices]) == 0:\n",
    "            # Esse nó é uma folha e passa a armazenar apenas seu valor\n",
    "            new_node = {\"value\": np.mean(y[region_indices])}\n",
    "        # Se não...\n",
    "        else:\n",
    "            # Faz uma nova divisão\n",
    "            new_node = get_split(X[region_indices], y[region_indices], available_features)\n",
    "            # E começa a crescer recursivamente a partir desse nó\n",
    "            recursive_growth(new_node, min_samples, max_depth, current_depth + 1, X[region_indices], y[region_indices], available_features)\n",
    "\n",
    "        # Se estamos falando da região da esquerda...\n",
    "        if region == \"left_region\":\n",
    "            # Adiciona esse novo nó como o filho da esquerda do nó atual\n",
    "            node['left'] = new_node\n",
    "        # Se não...\n",
    "        else:\n",
    "            # Adiciona-o como o filho da direita\n",
    "            node['right'] = new_node\n",
    "\n",
    "\n",
    "# Função para fazer a predição de um dado\n",
    "def predict_sample(node, sample):\n",
    "    # Enquanto não chegar a uma folha...\n",
    "    while \"value\" not in node.keys():\n",
    "        # Se o valor desse datapoint para a feature desse nó for menor que o limiar desse nó...\n",
    "        if sample[node[\"feature_index\"]] < node[\"tau\"]:\n",
    "            # Vai para o nó da esquerda\n",
    "            node = node[\"left\"]\n",
    "        # Se não...\n",
    "        else:\n",
    "            # Vai para o da direita\n",
    "            node = node[\"right\"]\n",
    "\n",
    "    # A previsão é o valor da folha alcançada\n",
    "    pred = node[\"value\"]\n",
    "\n",
    "    return pred\n",
    "\n",
    "# Função para fazer a predição de um conjunto de dados\n",
    "def predict(node, X):\n",
    "    # Número de dados\n",
    "    n_of_datapoints = X.shape[0]\n",
    "\n",
    "    # Array para armazenar as previsões\n",
    "    y_pred = np.zeros(n_of_datapoints)\n",
    "\n",
    "    # Para cada dado...\n",
    "    for datapoint in range(n_of_datapoints):\n",
    "        # Faz sua previsão e armazena-a no array\n",
    "        pred = predict_sample(node, X[datapoint])\n",
    "        y_pred[datapoint] = pred\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b166d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar uma floresta aleatória\n",
    "def train_random_forest(X, y, n_of_features, min_samples, max_depth, n_of_estimators):\n",
    "    # Criando os bags por meio do bootstrap\n",
    "    bags = bootstrap(X, n_of_estimators)\n",
    "    # Lista para armazenar as árvores\n",
    "    random_forest = []\n",
    "\n",
    "    # Para cada árvore...\n",
    "    for i in range(n_of_estimators):\n",
    "        # Sorteia as features que serão utilizadas\n",
    "        features = np.random.choice(np.array(range(X.shape[1])), n_of_features, replace = False)\n",
    "        # Filtra os dados apenas com os registros sorteados\n",
    "        filtered_X = X[bags[i]]\n",
    "        # Gera a árvore\n",
    "        root = get_split(filtered_X, y[bags[i]], features)\n",
    "        recursive_growth(root, min_samples, max_depth, 1, filtered_X, y[bags[i]], features)\n",
    "        # Salva-a na lista\n",
    "        random_forest.append(root)\n",
    "\n",
    "    return random_forest\n",
    "\n",
    "\n",
    "# Função para fazer a predição com base na floresta aleatória\n",
    "def predict_random_forest(random_forest, X):\n",
    "    # Matriz para armazenar as predições das árvores\n",
    "    preds = []\n",
    "\n",
    "    # Para cada árvore...\n",
    "    for i in range(len(random_forest)):\n",
    "        # Faz a predição dos dados\n",
    "        pred = predict(random_forest[i], X)\n",
    "        # Insere-a na matriz\n",
    "        preds.append(pred)\n",
    "\n",
    "    # Calcula a predição final\n",
    "    final_pred = aggregate_regression(preds)\n",
    "\n",
    "    return final_pred\n",
    "\n",
    "\n",
    "# Função para calcular o RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(((y_pred - y_true)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5354dd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.570810054159512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = train_random_forest(X_train, y_train, round(np.sqrt(X.shape[1])), 20, 6, 10)\n",
    "val_pred = predict_random_forest(random_forest, X_val)\n",
    "rmse(y_val, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f816bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pedro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Testando para várias configurações\n",
    "nums_features = [3, 4, 5]\n",
    "mins_samples = [15, 20, 25]\n",
    "max_depths = [4, 6, 8]\n",
    "nums_estimators = [5, 10, 15]\n",
    "\n",
    "best_configuration = [0, 0, 0, 0]\n",
    "best_error = np.inf\n",
    "\n",
    "for each_num_features in nums_features:\n",
    "    for each_min_samples in mins_samples:\n",
    "        for each_max_depth in max_depths:\n",
    "            for each_num_estimators in nums_estimators:\n",
    "                random_forest = train_random_forest(X_train, y_train, each_num_features, each_min_samples, each_max_depth, each_num_estimators)\n",
    "                val_pred = predict_random_forest(random_forest, X_val)\n",
    "                error = rmse(y_val, val_pred)\n",
    "                if error < best_error:\n",
    "                    best_configuration = [each_num_features, each_min_samples, each_max_depth, each_num_estimators]\n",
    "                    best_error = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93511528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best error: 3.3547049752519995\n",
      "  N° of features: 5\n",
      "  Min samples: 25\n",
      "  Max depth: 8\n",
      "  N° of estimators: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best error: {best_error}\\n  N° of features: {best_configuration[0]}\\n  Min samples: {best_configuration[1]}\\n  Max depth: {best_configuration[2]}\\n  N° of estimators: {best_configuration[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
